;; gorilla-repl.fileformat = 1

;; **
;;; # Detect Forum Leaf Node
;;; 
;;; This is a set of experiments on building a forum leaf detection module.
;; **

;; **
;;; ## 04/10:
;;; 
;;; Features:
;;; 1. jaccard similarity between anchor text and page-text
;;; 2. size of anchor-text language model
;;; 
;;;     Decision tree results:
;;;     Correctly Classified Instances          68               83.9506 %
;;;     Incorrectly Classified Instances        13               16.0494 %
;;;     Kappa statistic                          0.5979
;;;     Mean absolute error                      0.2647
;;;     Root mean squared error                  0.3638
;;;     Relative absolute error                 59.3884 %
;;;     Root relative squared error             77.1771 %
;;;     Total Number of Instances               81     
;;; 
;;; 
;;; So clearly this metric is not v good. Next idea I would like to try is:
;;; 
;;; * cosine similarity
;;; * remove links off the page and then attempt to perform this classification?
;;; * relationship between links and page content
;;; * date (o god fucking no)
;;; * anchor windows idea?
;;; 
;;; ## 04/12:
;;; 
;;; Adding more features to this sucker:
;;; 
;;; With stop word count as classifier
;;; 
;;; Train Error:
;;;     === Confusion Matrix ===
;;; 
;;;     a  b   <-- classified as
;;;     45  4 |  a = 1
;;;     4 50 |  b = 0
;;; 
;;;     === Summary ===
;;; 
;;;     Correctly Classified Instances          95               92.233  %
;;;     Incorrectly Classified Instances         8                7.767  %
;;;     Kappa statistic                          0.8443
;;;     Mean absolute error                      0.1369
;;;     Root mean squared error                  0.2617
;;;     Relative absolute error                 27.4502 %
;;;     Root relative squared error             52.3941 %
;;;     Total Number of Instances              103
;;;     
;;; 
;;; Test Error:
;;;     === Confusion Matrix ===
;;; 
;;;     a  b   <-- classified as
;;;     18  1 |  a = 1
;;;     6 16 |  b = 0
;;; 
;;;     === Summary ===
;;; 
;;;     Correctly Classified Instances          34               82.9268 %
;;;     Incorrectly Classified Instances         7               17.0732 %
;;;     Kappa statistic                          0.6627
;;;     Mean absolute error                      0.2189
;;;     Root mean squared error                  0.3845
;;;     Relative absolute error                 44.0037 %
;;;     Root relative squared error             77.1051 %
;;;     Total Number of Instances               41
;;;     
;;;     
;;; anchor text stop-words resulted a drop in perf. How about using how many stop words come from anchors vs the body (user-generated content links vs text i.e.)
;;; 
;;; the ratio also performs poorly:
;;; 
;;; 
;;; Train Error:
;;;     === Confusion Matrix ===
;;; 
;;;     a  b   <-- classified as
;;;     45  4 |  a = 1
;;;     3 51 |  b = 0
;;; 
;;;     === Summary ===
;;; 
;;;     Correctly Classified Instances          96               93.2039 %
;;;     Incorrectly Classified Instances         7                6.7961 %
;;;     Kappa statistic                          0.8636
;;;     Mean absolute error                      0.124 
;;;     Root mean squared error                  0.249 
;;;     Relative absolute error                 24.8506 %
;;;     Root relative squared error             49.8515 %
;;;     Total Number of Instances              103     
;;; 
;;; Test Error:
;;;     === Confusion Matrix ===
;;; 
;;;     a  b   <-- classified as
;;;     17  2 |  a = 1
;;;     6 16 |  b = 0
;;; 
;;;     === Summary ===
;;; 
;;;     Correctly Classified Instances          33               80.4878 %
;;;     Incorrectly Classified Instances         8               19.5122 %
;;;     Kappa statistic                          0.6132
;;;     Mean absolute error                      0.2357
;;;     Root mean squared error                  0.4222
;;;     Relative absolute error                 47.376  %
;;;     Root relative squared error             84.6694 %
;;;     Total Number of Instances               41  
;;;     
;;; Since this feature has also bombed what next?
;;; 
;;; 
;;; Also a couple more ideas, let us go try this exact feature set with an SVM
;;; 
;;; First we need to confirm what kind of error this is. If the guy is producing some errors that we don't like (for instance poor precision on just discussion pages but high accuracy on others)
;;; 
;;; Another feature I can think of is gap betweeen anchor tags (this is now getting to be fucking ridiculous)
;;; 
;;; 
;; **
